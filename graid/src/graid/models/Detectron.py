import os
import tempfile
import urllib.request
from collections.abc import Iterator
from itertools import islice
from pathlib import Path
from typing import Optional, Union

import cv2
import numpy as np
import torch
from detectron2 import model_zoo
from detectron2.checkpoint import DetectionCheckpointer
from detectron2.config import LazyConfig, get_cfg, instantiate
from detectron2.data import MetadataCatalog
from detectron2.data import transforms as T
from detectron2.engine import DefaultPredictor
from detectron2.structures import BitMasks
from detectron2.utils.logger import setup_logger
from detectron2.utils.visualizer import Visualizer
from graid.interfaces.InstanceSegmentationI import (
    InstanceSegmentationModelI,
    InstanceSegmentationResultI,
    Mask_Format,
)
from graid.interfaces.ObjectDetectionI import (
    BBox_Format,
    ObjectDetectionModelI,
    ObjectDetectionResultI,
)
from graid.utilities.common import convert_image_to_numpy, get_default_device
from PIL import Image

setup_logger()


def _resolve_cfg_file(path_or_url: str) -> str:
    """Resolve config file path, downloading if it's a URL."""
    if path_or_url.startswith("http://") or path_or_url.startswith("https://"):
        # Download to a temporary file
        local_path = tempfile.NamedTemporaryFile(
            delete=False, suffix=".py" if path_or_url.endswith(".py") else ".yaml"
        ).name
        try:
            urllib.request.urlretrieve(path_or_url, local_path)
            return local_path
        except Exception as e:
            raise RuntimeError(
                f"Failed to download Detectron2 config from {path_or_url}: {e}"
            )
    elif os.path.isfile(path_or_url):
        return path_or_url
    else:
        # Treat as model_zoo shorthand
        return model_zoo.get_config_file(path_or_url)


class DetectronBase:
    """Base class for Detectron2 models with shared functionality."""

    def __init__(
        self,
        config_file: str,
        weights_file: str,
        threshold: float = 0.5,
        device: Optional[Union[str, torch.device]] = None,
    ):
        # ------------------------------------------------------------------
        # Input Detectron2 config & weights â€“ support either:
        #   1) Built-in model_zoo shorthand (e.g. "COCO-InstanceSegmentation/...yaml")
        #   2) Local absolute/relative file path
        #   3) Remote HTTP(S) URL (auto-download to a temp file)
        # ------------------------------------------------------------------

        cfg_path = _resolve_cfg_file(config_file)

        # ---- setup config -----------------------------------------------
        if cfg_path.endswith(".py"):
            # Use LazyConfig for .py files
            cfg = LazyConfig.load_file(cfg_path)
            cfg.model.device = (
                str(get_default_device()) if device is None else str(device)
            )
            cfg.model.roi_heads.box_predictor.test_score_thresh = threshold
        else:
            # Use traditional config for .yaml files
            cfg = get_cfg()
            # allow config files to introduce new keys (e.g. custom backbones)
            if hasattr(cfg, "set_new_allowed"):
                cfg.set_new_allowed(True)
            else:
                cfg.new_allowed = True
            cfg.MODEL.DEVICE = (
                str(get_default_device()) if device is None else str(device)
            )
            cfg.merge_from_file(cfg_path)
            cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = threshold

        # ---- resolve weights ---------------------------------------------
        if weights_file.startswith("http://") or weights_file.startswith("https://"):
            if cfg_path.endswith(".py"):
                cfg.model.weights = weights_file  # LazyConfig
            else:
                cfg.MODEL.WEIGHTS = weights_file  # traditional config
        elif os.path.isfile(weights_file):
            if cfg_path.endswith(".py"):
                cfg.model.weights = weights_file  # LazyConfig
            else:
                cfg.MODEL.WEIGHTS = weights_file  # traditional config
        else:
            # treat as model_zoo shorthand (will raise if unavailable)
            weights_url = model_zoo.get_checkpoint_url(weights_file)
            if cfg_path.endswith(".py"):
                cfg.model.weights = weights_url  # LazyConfig
            else:
                cfg.MODEL.WEIGHTS = weights_url  # traditional config

        # ---- create predictor --------------------------------------------
        if cfg_path.endswith(".py"):
            # For LazyConfig, create a traditional config for DefaultPredictor
            # DefaultPredictor expects a traditional CfgNode, not LazyConfig
            traditional_cfg = get_cfg()
            traditional_cfg.MODEL.DEVICE = (
                str(get_default_device()) if device is None else str(device)
            )
            traditional_cfg.MODEL.WEIGHTS = cfg.model.weights
            traditional_cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = threshold
            # Copy other essential config values
            traditional_cfg.MODEL.META_ARCHITECTURE = "GeneralizedRCNN"
            traditional_cfg.MODEL.BACKBONE.NAME = "RegNet"
            traditional_cfg.MODEL.ROI_HEADS.NAME = "StandardROIHeads"
            traditional_cfg.MODEL.ROI_HEADS.NUM_CLASSES = 80  # COCO classes
            traditional_cfg.INPUT.FORMAT = "BGR"
            self._predictor = DefaultPredictor(traditional_cfg)
        else:
            self._predictor = DefaultPredictor(cfg)

        # ---- metadata ----------------------------------------------------
        if cfg_path.endswith(".py"):
            # For LazyConfig, we need to handle metadata differently
            try:
                self._metadata = MetadataCatalog.get(
                    cfg.dataloader.train.dataset.names[0]
                )
            except (KeyError, IndexError, AttributeError) as e:
                # Fallback to COCO metadata if dataset metadata not available
                logger.warning(
                    f"Could not get dataset metadata, using COCO fallback: {e}"
                )
                self._metadata = MetadataCatalog.get("coco_2017_train")
        else:
            self._metadata = MetadataCatalog.get(cfg.DATASETS.TRAIN[0])

        # Store config and other attributes
        self.cfg = cfg
        self.model_name = config_file
        self.threshold = threshold

        # Store config for cleanup
        self._cfg_path = cfg_path
        self._config_file = config_file

    def to(self, device: Union[str, torch.device]):
        """Move model to specified device."""
        # Update config device
        self.cfg.MODEL.DEVICE = str(device)
        # Recreate predictor with new device
        self._predictor = DefaultPredictor(self.cfg)

    def set_threshold(self, threshold: float):
        """Set confidence threshold for detections."""
        self.threshold = threshold
        self.cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = threshold
        # Recreate predictor with new threshold
        self._predictor = DefaultPredictor(self.cfg)

    def __str__(self):
        return self.model_name.split("/")[-1].split(".")[0]


class Detectron_obj(DetectronBase, ObjectDetectionModelI):
    def __init__(
        self,
        config_file: str,
        weights_file: str,
        threshold: float = 0.0,
        device: Optional[Union[str, torch.device]] = None,
    ):
        super().__init__(config_file, weights_file, threshold, device)

    def identify_for_image(self, image, **kwargs) -> list[ObjectDetectionResultI]:
        """
        Run object detection on an image or a batch of images.
        Args:
            image: either a PIL image or a tensor of shape (B, C, H, W)
                where B is the batch size, C is the channel size, H is the
                height, and W is the width.
        Returns:
            A list of list of ObjectDetectionResultI, where the outer list
            represents the batch of images, and the inner list represents the
            detections in a particular image.
        """
        image = convert_image_to_numpy(image)

        # TODO: Detectron2 predictor does not support batched inputs
        #  so either we loop through the batch or we do the preprocessing steps
        #  of the predictor ourselves and then call the model
        #  I prefer the latter approach. Preprocessing steps are in the predictor:
        #   - load the checkpoint
        #   - take the image in BGR format and apply conversion defined by cfg.INPUT.FORMAT
        #   - resize the image

        if isinstance(image, torch.Tensor):
            # Convert to HWC (Numpy format) if image is Pytorch tensor in CHW format
            if image.ndimension() == 4:  # Batched input (B, C, H, W)
                batch_results = []
                for img in image:
                    img_np = img.permute(1, 2, 0).cpu().numpy()  # Convert to HWC
                    batch_results.append(self._process_single_image(img_np))
                return batch_results

            elif image.ndimension() == 3:  # Single input (C, H, W)
                print(f"image should be CHW: {image.shape}")
                image = image.permute(1, 2, 0).cpu().numpy()  # Convert to HWC
                # Ensure the array is contiguous in memory
                image = np.ascontiguousarray(image)

        # Single image input
        print(f"image should be HWC: {image.shape}")
        return self._process_single_image(image)

    def _process_single_image(self, image: np.ndarray) -> list[ObjectDetectionResultI]:
        predictions = self._predictor(image)

        if len(predictions) == 0:
            print("Predictions were empty and not found in this image.")
            return []

        if "instances" not in predictions or len(predictions["instances"]) == 0:
            print("No instances or predictions in this image.")
            return []

        instances = predictions["instances"]

        if not hasattr(instances, "pred_boxes") or len(instances.pred_boxes) == 0:
            print("Prediction boxes attribute missing or not found in instances.")
            return []

        formatted_results = []
        for i in range(len(instances)):
            box = instances.pred_boxes[i].tensor.cpu().numpy().tolist()[0]
            score = instances.scores[i].item()
            cls_id = int(instances.pred_classes[i].item())
            label = self._metadata.thing_classes[cls_id]

            odr = ObjectDetectionResultI(
                score=score,
                cls=cls_id,
                label=label,
                bbox=box,
                image_hw=image.shape[:2],
                bbox_format=BBox_Format.XYXY,
            )

            formatted_results.append(odr)

        return formatted_results

    def identify_for_image_batch(
        self, batched_images, debug: bool = False, **kwargs
    ) -> list[ObjectDetectionResultI]:
        assert (
            batched_images.ndimension() == 4
        ), "Input tensor must be of shape (B, C, H, W) in RGB format"
        batched_images = batched_images[:, [2, 1, 0], ...]  # Convert RGB to BGR
        list_of_images = []
        with torch.no_grad():  # https://github.com/sphinx-doc/sphinx/issues/4258
            for i in range(batched_images.shape[0]):
                image = batched_images[i]
                image = image.permute(1, 2, 0).cpu().numpy()  # Convert to HWC
                image = self._predictor.aug.get_transform(image).apply_image(image)
                image = torch.as_tensor(
                    image.astype("float32").transpose(2, 0, 1)
                )  # Convert back to CHW
                image = image.to(self.cfg.MODEL.DEVICE).detach()
                height, width = image.shape[1:]
                list_of_images.append(
                    {"image": image, "height": height, "width": width}
                )

            predictions = self._predictor.model(list_of_images)

        formatted_results = []
        for i in range(len(predictions)):
            img_result = []
            for j in range(len(predictions[i]["instances"])):
                box = (
                    predictions[i]["instances"][j]
                    .pred_boxes.tensor.cpu()
                    .numpy()
                    .tolist()[0]
                )
                score = predictions[i]["instances"][j].scores.item()
                cls_id = int(predictions[i]["instances"][j].pred_classes.item())
                label = self._metadata.thing_classes[cls_id]

                odr = ObjectDetectionResultI(
                    score=score,
                    cls=cls_id,
                    label=label,
                    bbox=box,
                    image_hw=(height, width),
                    bbox_format=BBox_Format.XYXY,
                )

                img_result.append(odr)

            formatted_results.append(img_result)

        return formatted_results

    def identify_for_video(
        self,
        video: Union[Iterator[Image.Image], list[Image.Image]],
        batch_size: int = 1,
    ) -> Iterator[list[list[ObjectDetectionResultI]]]:
        """
        Run object detection on a video represented as an iterator or list of images.
        Args:
            video: An iterator or list of PIL images.
            batch_size: Number of images to process at a time.
        Returns:
            An iterator of lists of lists of ObjectDetectionResultI, where the outer
            list represents the batches, the middle list represents frames, and the
            inner list represents detections within a frame.
        """

        def batch_iterator(iterable, n):
            iterator = iter(iterable)
            return iter(lambda: list(islice(iterator, n)), [])

        video_iterator = batch_iterator(video, batch_size)

        for batch in video_iterator:
            if not batch:  # End of iterator
                break

            batch_results = []
            for image in batch:
                image = convert_image_to_numpy(image)
                frame_results = self._process_single_image(image)
                batch_results.append(frame_results)

            yield batch_results


class Detectron_seg(DetectronBase, InstanceSegmentationModelI):
    def __init__(
        self,
        config_file: str,
        weights_file: str,
        threshold: float = 0.0,
        device: Optional[Union[str, torch.device]] = None,
    ):
        super().__init__(config_file, weights_file, threshold, device)

    def identify_for_image(
        self,
        image: Union[
            str, Path, int, Image.Image, list, tuple, np.ndarray, torch.Tensor
        ],
        debug: bool = False,
        **kwargs,
    ) -> list[InstanceSegmentationResultI]:
        """
        Run instance segmentation on an image.
        Args:
            image: Input image as PIL Image, numpy array, or tensor
        Returns:
            A list of InstanceSegmentationResultI objects
        """
        image = convert_image_to_numpy(image)
        return self._process_single_image(image)

    def _process_single_image(
        self, image: np.ndarray
    ) -> list[InstanceSegmentationResultI]:
        """Process a single image for instance segmentation."""
        predictions = self._predictor(image)

        if len(predictions) == 0:
            print("Predictions were empty and not found in this image.")
            return []

        if "instances" not in predictions or len(predictions["instances"]) == 0:
            print("No instances or predictions in this image.")
            return []

        instances = predictions["instances"]

        if not hasattr(instances, "pred_masks") or len(instances.pred_masks) == 0:
            print("Prediction masks attribute missing or not found in instances.")
            return []

        formatted_results = []
        height, width = image.shape[:2]

        for i in range(len(instances)):
            score = instances.scores[i].item()
            cls_id = int(instances.pred_classes[i].item())
            label = self._metadata.thing_classes[cls_id]
            mask = instances.pred_masks[i]

            # Create BitMasks object from the mask tensor
            bitmask = BitMasks(mask.unsqueeze(0))

            result = InstanceSegmentationResultI(
                score=score,
                cls=cls_id,
                label=label,
                instance_id=i,
                mask=bitmask,
                image_hw=(height, width),
                mask_format=Mask_Format.BITMASK,
            )

            formatted_results.append(result)

        return formatted_results

    def identify_for_image_batch(
        self,
        image: Union[
            str, Path, int, Image.Image, list, tuple, np.ndarray, torch.Tensor
        ],
        debug: bool = False,
        **kwargs,
    ) -> list[list[InstanceSegmentationResultI]]:
        """
        Run instance segmentation on a batch of images.
        Args:
            image: Batched images as tensor of shape (B, C, H, W)
        Returns:
            A list of lists of InstanceSegmentationResultI objects
        """

        if isinstance(image, torch.Tensor):
            assert (
                image.ndimension() == 4
            ), "Input tensor must be of shape (B, C, H, W) in RGB format"

            # Convert RGB to BGR and prepare images for model
            batched_images = image[:, [2, 1, 0], ...]  # Convert RGB to BGR
            list_of_images = []

            with torch.no_grad():
                for i in range(batched_images.shape[0]):
                    img = batched_images[i]
                    img = img.permute(1, 2, 0).cpu().numpy()  # Convert to HWC

                    # Apply preprocessing transformations from predictor
                    img = self._predictor.aug.get_transform(img).apply_image(img)
                    img = torch.as_tensor(
                        img.astype("float32").transpose(2, 0, 1)
                    )  # Convert back to CHW
                    img = img.to(self.cfg.MODEL.DEVICE).detach()

                    height, width = img.shape[1:]
                    list_of_images.append(
                        {"image": img, "height": height, "width": width}
                    )

                # Process entire batch through model at once
                predictions = self._predictor.model(list_of_images)

            # Format results for each image in batch
            formatted_results = []
            for i in range(len(predictions)):
                img_results = []

                if (
                    "instances" not in predictions[i]
                    or len(predictions[i]["instances"]) == 0
                ):
                    formatted_results.append(img_results)
                    continue

                instances = predictions[i]["instances"]

                if (
                    not hasattr(instances, "pred_masks")
                    or len(instances.pred_masks) == 0
                ):
                    formatted_results.append(img_results)
                    continue

                height = list_of_images[i]["height"]
                width = list_of_images[i]["width"]

                for j in range(len(instances)):
                    score = instances.scores[j].item()
                    cls_id = int(instances.pred_classes[j].item())
                    label = self._metadata.thing_classes[cls_id]
                    mask = instances.pred_masks[j]

                    # Create BitMasks object from the mask tensor
                    bitmask = BitMasks(mask.unsqueeze(0))

                    result = InstanceSegmentationResultI(
                        score=score,
                        cls=cls_id,
                        label=label,
                        instance_id=j,
                        mask=bitmask,
                        image_hw=(height, width),
                        mask_format=Mask_Format.BITMASK,
                    )

                    img_results.append(result)

                formatted_results.append(img_results)

            return formatted_results
        else:
            # Single image case
            return [self.identify_for_image(image, debug=debug, **kwargs)]

    def identify_for_video(
        self,
        video: Union[Iterator[Image.Image], list[Image.Image]],
        batch_size: int = 1,
    ) -> Iterator[list[InstanceSegmentationResultI]]:
        """
        Run instance segmentation on a video represented as iterator/list of images.
        Args:
            video: An iterator or list of PIL images
            batch_size: Number of images to process at a time
        Returns:
            An iterator of lists of InstanceSegmentationResultI objects
        """

        def batch_iterator(iterable, n):
            iterator = iter(iterable)
            return iter(lambda: list(islice(iterator, n)), [])

        video_iterator = batch_iterator(video, batch_size)

        for batch in video_iterator:
            if not batch:  # End of iterator
                break

            for image in batch:
                image = convert_image_to_numpy(image)
                frame_results = self._process_single_image(image)
                yield frame_results

    def visualize(self, image: Union[np.ndarray, torch.Tensor]):
        """Visualize segmentation results on an image."""
        # Local import to avoid loading matplotlib unless visualization is needed
        import matplotlib.pyplot as plt

        image = convert_image_to_numpy(image)
        outputs = self._predictor(image)
        v = Visualizer(image[:, :, ::-1], self._metadata, scale=1.2)
        v = v.draw_instance_predictions(outputs["instances"].to("cpu"))
        plt.figure(figsize=(14, 10))
        plt.imshow(cv2.cvtColor(v.get_image()[:, :, ::-1], cv2.COLOR_BGR2RGB))
        plt.show()


class DetectronLazy(ObjectDetectionModelI):
    """
    Detectron2 model using a Python-based 'lazy' config file.
    This is common for newer models like ViTDet.
    """

    def __init__(
        self,
        config_file: str,
        weights_file: str,
        threshold: float = 0.5,
        device: Optional[Union[str, torch.device]] = None,
    ):
        self.device = device if device is not None else get_default_device()
        self.threshold = threshold

        # Load lazy config
        cfg = LazyConfig.load(config_file)

        # Set score threshold for Cascade R-CNN, which has multiple roi_heads
        if hasattr(cfg.model, "roi_heads") and hasattr(cfg.model.roi_heads, "box_head"):
            # It's a cascade model, iterate through stages
            if isinstance(cfg.model.roi_heads.box_head, list):
                for head in cfg.model.roi_heads.box_head:
                    if hasattr(head, "test_score_thresh"):
                        head.test_score_thresh = threshold
            else:  # It's a single head
                if hasattr(cfg.model.roi_heads, "box_predictor"):
                    if hasattr(cfg.model.roi_heads.box_predictor, "test_score_thresh"):
                        cfg.model.roi_heads.box_predictor.test_score_thresh = threshold

        # Build model
        self.model = instantiate(cfg.model)
        self.model.to(self.device)
        self.model.eval()

        # Load checkpoint
        checkpointer = DetectionCheckpointer(self.model)
        checkpointer.load(weights_file)

        self.cfg = cfg
        self.model_name = Path(config_file).stem

        # Get preprocessing info from config, with defaults
        self.short_edge_length = 800
        self.max_size = 1333
        try:
            # This path might differ for other lazy configs
            aug = cfg.dataloader.test.mapper.augmentations[0]
            if aug.short_edge_length and aug.max_size:
                self.short_edge_length = aug.short_edge_length
                self.max_size = aug.max_size
        except (AttributeError, IndexError, KeyError):
            pass  # Use defaults

        # Get metadata for class names
        try:
            # This path might differ for other lazy configs
            dataset_names = cfg.dataloader.test.dataset.names
            self.metadata = MetadataCatalog.get(dataset_names)
        except (AttributeError, IndexError, KeyError):
            print(
                "Warning: Could not find dataset metadata in config. Fallback to COCO."
            )
            self.metadata = MetadataCatalog.get("coco_2017_train")

    def to(self, device: Union[str, torch.device]):
        """Move model to specified device."""
        self.device = device
        self.model.to(self.device)

    def set_threshold(self, threshold: float):
        """Set confidence threshold for detections."""
        self.threshold = threshold
        # Also update the running model config
        if hasattr(self.cfg.model, "roi_heads") and hasattr(
            self.cfg.model.roi_heads, "box_head"
        ):
            if isinstance(self.cfg.model.roi_heads.box_head, list):
                for head in self.cfg.model.roi_heads.box_head:
                    if hasattr(head, "test_score_thresh"):
                        head.test_score_thresh = threshold
            else:
                if hasattr(self.cfg.model.roi_heads, "box_predictor"):
                    if hasattr(
                        self.cfg.model.roi_heads.box_predictor, "test_score_thresh"
                    ):
                        self.cfg.model.roi_heads.box_predictor.test_score_thresh = (
                            threshold
                        )

    def __str__(self):
        return self.model_name

    def identify_for_image_batch(
        self, batched_images, debug: bool = False, **kwargs
    ) -> list[list[ObjectDetectionResultI]]:
        assert (
            batched_images.ndimension() == 4
        ), "Input tensor must be of shape (B, C, H, W) in RGB format"

        list_of_inputs = []
        original_shapes = []

        for i in range(batched_images.shape[0]):
            image_tensor_chw_rgb = batched_images[i]

            # Convert to numpy HWC RGB
            image_np_hwc_rgb = image_tensor_chw_rgb.permute(1, 2, 0).cpu().numpy()

            # Convert RGB to BGR for model
            image_np_hwc_bgr = cv2.cvtColor(image_np_hwc_rgb, cv2.COLOR_RGB2BGR)

            original_height, original_width = image_np_hwc_bgr.shape[:2]
            original_shapes.append((original_height, original_width))

            transform_gen = T.ResizeShortestEdge(
                [self.short_edge_length, self.short_edge_length], self.max_size
            )

            transformed_image = transform_gen.get_transform(
                image_np_hwc_bgr
            ).apply_image(image_np_hwc_bgr)
            transformed_image_tensor = torch.as_tensor(
                transformed_image.astype("float32").transpose(2, 0, 1)
            )

            inputs = {
                "image": transformed_image_tensor.to(self.device),
                "height": original_height,
                "width": original_width,
            }
            list_of_inputs.append(inputs)

        with torch.no_grad():
            predictions = self.model(list_of_inputs)

        formatted_results = []
        for i, prediction in enumerate(predictions):
            img_result = []
            instances = prediction["instances"]
            image_hw = original_shapes[i]

            if len(instances) > 0:
                for j in range(len(instances)):
                    box = instances.pred_boxes[j].tensor.cpu().numpy().tolist()[0]
                    score = instances.scores[j].item()
                    cls_id = int(instances.pred_classes[j].item())
                    label = self.metadata.thing_classes[cls_id]

                    odr = ObjectDetectionResultI(
                        score=score,
                        cls=cls_id,
                        label=label,
                        bbox=box,
                        image_hw=image_hw,
                        bbox_format=BBox_Format.XYXY,
                    )
                    img_result.append(odr)

            formatted_results.append(img_result)

        return formatted_results

    def identify_for_image(self, image, **kwargs) -> list[ObjectDetectionResultI]:
        """Runs detection on a single image."""
        numpy_image = convert_image_to_numpy(image)  # This should be RGB HWC
        # to tensor, CHW
        image_tensor = torch.from_numpy(
            np.ascontiguousarray(numpy_image.transpose(2, 0, 1))
        )
        if image_tensor.ndimension() == 3:
            image_tensor = image_tensor.unsqueeze(0)

        results_batch = self.identify_for_image_batch(image_tensor, **kwargs)
        return results_batch[0] if results_batch else []

    def identify_for_video(
        self,
        video: Union[Iterator[Image.Image], list[Image.Image]],
        batch_size: int = 1,
    ) -> Iterator[list[list[ObjectDetectionResultI]]]:
        """
        Run object detection on a video represented as an iterator or list of images.
        Args:
            video: An iterator or list of PIL images.
            batch_size: Number of images to process at a time.
        Returns:
            An iterator of lists of lists of ObjectDetectionResultI, where the outer
            list represents the batches, the middle list represents frames, and the
            inner list represents detections within a frame.
        """

        def batch_iterator(iterable, n):
            iterator = iter(iterable)
            return iter(lambda: list(islice(iterator, n)), [])

        video_iterator = batch_iterator(video, batch_size)

        for batch in video_iterator:
            if not batch:  # End of iterator
                break

            # Convert all images in batch to numpy arrays
            numpy_batch = [convert_image_to_numpy(img) for img in batch]

            # Convert to a tensor (B, H, W, C) -> (B, C, H, W)
            tensor_batch = torch.from_numpy(np.array(numpy_batch)).permute(0, 3, 1, 2)

            batch_results = self.identify_for_image_batch(tensor_batch)
            yield batch_results
